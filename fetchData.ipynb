{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def uprint(*objects, sep=' ', end='\\n', file=sys.stdout):\n",
    "    \"\"\" Wrapper function around print from Stackoverflow\n",
    "    https://stackoverflow.com/questions/14630288/unicodeencodeerror-charmap-codec-cant-encode-character-maps-to-undefined/16120218\n",
    "    \"\"\"\n",
    "    enc = file.encoding\n",
    "    if enc == 'UTF-8':\n",
    "        print(*objects, sep=sep, end=end, file=file)\n",
    "    else:\n",
    "        f = lambda obj: str(obj).encode(enc, errors='backslashreplace').decode(enc)\n",
    "        print(*map(f, objects), sep=sep, end=end, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsers\n",
    "import csv \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_stat_names(dict_of_stats):\n",
    "    \"\"\" Extracts all the names of the statistics\n",
    "    Args:\n",
    "        dict_of_stats (dict): Dictionary containing key-alue pair of stats\n",
    "    \"\"\"\n",
    "    stat_names = []\n",
    "    for key, val in dict_of_stats.items():\n",
    "        stat_names += [key]\n",
    "    return stat_names\n",
    "\n",
    "def parse_players(list_of_players, base_filename):\n",
    "    stat_names = extract_stat_names(list_of_players[0])\n",
    "    filename = base_filename + 'players_raw.csv'\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    f = open(filename, 'w+', encoding='utf8', newline='')\n",
    "    w = csv.DictWriter(f, sorted(stat_names))\n",
    "    w.writeheader()\n",
    "    for player in list_of_players:\n",
    "            w.writerow({k:str(v).encode('utf-8').decode('utf-8') for k, v in player.items()})\n",
    "\n",
    "def parse_player_history(list_of_histories, base_filename, player_name, Id):\n",
    "    if len(list_of_histories) > 0:\n",
    "        stat_names = extract_stat_names(list_of_histories[0])\n",
    "        filename = base_filename + player_name + '_' + str(Id) + '/history.csv'\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        f = open(filename, 'w+', encoding='utf8', newline='')\n",
    "        w = csv.DictWriter(f, sorted(stat_names))\n",
    "        w.writeheader()\n",
    "        for history in list_of_histories:\n",
    "            w.writerow(history)\n",
    "\n",
    "def parse_player_gw_history(list_of_gw, base_filename, player_name, Id):\n",
    "    if len(list_of_gw) > 0:\n",
    "        stat_names = extract_stat_names(list_of_gw[0])\n",
    "        filename = base_filename + player_name + '_' + str(Id) + '/gw.csv'\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        f = open(filename, 'w+', encoding='utf8', newline='')\n",
    "        w = csv.DictWriter(f, sorted(stat_names))\n",
    "        w.writeheader()\n",
    "        for gw in list_of_gw:\n",
    "            w.writerow(gw)\n",
    "\n",
    "def parse_gw_entry_history(data, outfile_base):\n",
    "    for gw in data:\n",
    "        picks = gw['picks']\n",
    "        event = gw['entry_history']['event']\n",
    "        filename = \"picks_\" +str(event) + \".csv\"\n",
    "        picks_df = pd.DataFrame.from_records(picks)\n",
    "        picks_df.to_csv(os.path.join(outfile_base, filename), index=False)\n",
    "\n",
    "def parse_entry_history(data, outfile_base):\n",
    "    chips_df = pd.DataFrame.from_records(data[\"chips\"])\n",
    "    chips_df.to_csv(os.path.join(outfile_base, 'chips.csv'))\n",
    "    season_df = pd.DataFrame.from_records(data[\"past\"])\n",
    "    season_df.to_csv(os.path.join(outfile_base, 'history.csv'))\n",
    "    #profile_data = data[\"entry\"].pop('kit', data[\"entry\"])\n",
    "    #profile_df = pd.DataFrame.from_records(profile_data)\n",
    "    #profile_df.to_csv(os.path.join(outfile_base, 'profile.csv'))\n",
    "    gw_history_df = pd.DataFrame.from_records(data[\"current\"])\n",
    "    gw_history_df.to_csv(os.path.join(outfile_base, 'gws.csv'), index=False)\n",
    "\n",
    "def parse_entry_leagues(data, outfile_base):\n",
    "    classic_leagues_df = pd.DataFrame.from_records(data[\"leagues\"][\"classic\"])\n",
    "    classic_leagues_df.to_csv(os.path.join(outfile_base, 'classic_leagues.csv'))\n",
    "    try:\n",
    "        cup_leagues_df = pd.DataFrame.from_records(data[\"leagues\"][\"cup\"][\"matches\"])\n",
    "        cup_leagues_df.to_csv(os.path.join(outfile_base, 'cup_leagues.csv'))\n",
    "    except KeyError:\n",
    "        print(\"No cups yet\")\n",
    "    h2h_leagues_df = pd.DataFrame.from_records(data[\"leagues\"][\"h2h\"])\n",
    "    h2h_leagues_df.to_csv(os.path.join(outfile_base, 'h2h_leagues.csv'))\n",
    "\n",
    "def parse_transfer_history(data, outfile_base):\n",
    "    wildcards_df = pd.DataFrame.from_records(data[\"wildcards\"])\n",
    "    wildcards_df.to_csv(os.path.join(outfile_base, 'wildcards.csv'))\n",
    "    transfers_df = pd.DataFrame.from_records(data[\"history\"])\n",
    "    transfers_df.to_csv(os.path.join(outfile_base, 'transfers.csv'))\n",
    "\n",
    "def parse_fixtures(data, outfile_base):\n",
    "    fixtures_df = pd.DataFrame.from_records(data)\n",
    "    fixtures_df.to_csv(os.path.join(outfile_base, 'fixtures.csv'), index=False)\n",
    "\n",
    "def parse_team_data(data, outfile_base):\n",
    "    teams_df = pd.DataFrame.from_records(data)\n",
    "    teams_df.to_csv(os.path.join(outfile_base, 'teams.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaners\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "\n",
    "def clean_players(filename, base_filename):\n",
    "    \"\"\" Creates a file with only important data columns for each player\n",
    "    Args:\n",
    "        filename (str): Name of the file that contains the full data for each player\n",
    "    \"\"\"\n",
    "    headers = ['first_name', 'second_name', 'goals_scored', 'assists', 'total_points', 'minutes', 'goals_conceded', 'creativity', 'influence', 'threat', 'bonus', 'bps', 'ict_index', 'clean_sheets', 'red_cards', 'yellow_cards', 'selected_by_percent', 'now_cost']\n",
    "    fin = open(filename, 'r+', encoding='utf-8')\n",
    "    outname = base_filename + 'cleaned_players.csv'\n",
    "    os.makedirs(os.path.dirname(outname), exist_ok=True)\n",
    "    fout = open(outname, 'w+', encoding='utf-8', newline='')\n",
    "    reader = csv.DictReader(fin)\n",
    "    writer = csv.DictWriter(fout, headers, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        writer.writerow(line)\n",
    "\n",
    "def id_players(players_filename, base_filename):\n",
    "    \"\"\" Creates a file that contains the name to id mappings for each player\n",
    "    Args:\n",
    "        players_filename (str): Name of the file that contains the full data for each player\n",
    "    \"\"\"\n",
    "    headers = ['first_name', 'second_name', 'id']\n",
    "    fin = open(players_filename, 'r+', encoding='utf-8')\n",
    "    outname = base_filename + 'player_idlist.csv'\n",
    "    os.makedirs(os.path.dirname(outname), exist_ok=True)\n",
    "    fout = open(outname, 'w+', encoding='utf-8', newline='')\n",
    "    reader = csv.DictReader(fin)\n",
    "    writer = csv.DictWriter(fout, headers, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        writer.writerow(line)\n",
    "\n",
    "def get_player_ids(base_filename):\n",
    "    \"\"\" Gets the list of all player ids and player names\n",
    "    \"\"\"\n",
    "    filename = base_filename + 'player_idlist.csv'\n",
    "    fin = open(filename, 'r+', encoding='utf-8')\n",
    "    reader = csv.DictReader(fin)\n",
    "    player_ids = {}\n",
    "    for line in reader:\n",
    "        k = int(line['id'])\n",
    "        v = line['first_name'] + '_' + line['second_name']\n",
    "        player_ids[k] = v\n",
    "    return player_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_data():\n",
    "    \"\"\" Retrieve the fpl player data from the hard-coded url\n",
    "    \"\"\"\n",
    "    response = requests.get(\"https://fantasy.premierleague.com/api/bootstrap-static/\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Response was code \" + str(response.status_code))\n",
    "    responseStr = response.text\n",
    "    data = json.loads(responseStr)\n",
    "    return data\n",
    "\n",
    "def get_individual_player_data(player_id):\n",
    "    \"\"\" Retrieve the player-specific detailed data\n",
    "    Args:\n",
    "        player_id (int): ID of the player whose data is to be retrieved\n",
    "    \"\"\"\n",
    "    base_url = \"https://fantasy.premierleague.com/api/element-summary/\"\n",
    "    full_url = base_url + str(player_id) + \"/\"\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        try:\n",
    "            response = requests.get(full_url)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Response was code \" + str(response.status_code))\n",
    "    data = json.loads(response.text)\n",
    "    return data\n",
    "\n",
    "def get_entry_data(entry_id):\n",
    "    \"\"\" Retrieve the summary/history data for a specific entry/team\n",
    "    Args:\n",
    "        entry_id (int) : ID of the team whose data is to be retrieved\n",
    "    \"\"\"\n",
    "    base_url = \"https://fantasy.premierleague.com/api/entry/\"\n",
    "    full_url = base_url + str(entry_id) + \"/history/\"\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        try:\n",
    "            response = requests.get(full_url)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Response was code \" + str(response.status_code))\n",
    "    data = json.loads(response.text)\n",
    "    return data\n",
    "\n",
    "def get_entry_personal_data(entry_id):\n",
    "    \"\"\" Retrieve the summary/history data for a specific entry/team\n",
    "    Args:\n",
    "        entry_id (int) : ID of the team whose data is to be retrieved\n",
    "    \"\"\"\n",
    "    base_url = \"https://fantasy.premierleague.com/api/entry/\"\n",
    "    full_url = base_url + str(entry_id) + \"/\"\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        try:\n",
    "            response = requests.get(full_url)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Response was code \" + str(response.status_code))\n",
    "    data = json.loads(response.text)\n",
    "    return data\n",
    "\n",
    "def get_entry_gws_data(entry_id,num_gws):\n",
    "    \"\"\" Retrieve the gw-by-gw data for a specific entry/team\n",
    "    Args:\n",
    "        entry_id (int) : ID of the team whose data is to be retrieved\n",
    "    \"\"\"\n",
    "    base_url = \"https://fantasy.premierleague.com/api/entry/\"\n",
    "    gw_data = []\n",
    "    for i in range(1, num_gws+1):\n",
    "        full_url = base_url + str(entry_id) + \"/event/\" + str(i) + \"/picks/\"\n",
    "        response = ''\n",
    "        while response == '':\n",
    "            try:\n",
    "                response = requests.get(full_url)\n",
    "            except:\n",
    "                time.sleep(5)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\"Response was code \" + str(response.status_code))\n",
    "        data = json.loads(response.text)\n",
    "        gw_data += [data]\n",
    "    return gw_data\n",
    "\n",
    "def get_entry_transfers_data(entry_id):\n",
    "    \"\"\" Retrieve the transfer data for a specific entry/team\n",
    "    Args:\n",
    "        entry_id (int) : ID of the team whose data is to be retrieved\n",
    "    \"\"\"\n",
    "    base_url = \"https://fantasy.premierleague.com/api/entry/\"\n",
    "    full_url = base_url + str(entry_id) + \"/transfers/\"\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        try:\n",
    "            response = requests.get(full_url)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Response was code \" + str(response.status_code))\n",
    "    data = json.loads(response.text)\n",
    "    return data\n",
    "\n",
    "def get_fixtures_data():\n",
    "    \"\"\" Retrieve the fixtures data for the season\n",
    "    \"\"\"\n",
    "    url = \"https://fantasy.premierleague.com/api/fixtures/\"\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Response was code \" + str(response.status_code))\n",
    "    data = json.loads(response.text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "def merge_gw(gw, gw_directory):\n",
    "    merged_gw_filename = \"merged_gw.csv\"\n",
    "    gw_filename = \"gw\" + str(gw) + \".csv\"\n",
    "    gw_path = os.path.join(gw_directory, gw_filename)\n",
    "    fin = open(gw_path, 'rU', encoding=\"utf-8\")\n",
    "    reader = csv.DictReader(fin)\n",
    "    fieldnames = reader.fieldnames\n",
    "    fieldnames += [\"GW\"]\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        row[\"GW\"] = gw\n",
    "        rows += [row]\n",
    "    out_path = os.path.join(gw_directory, merged_gw_filename)\n",
    "    fout = open(out_path,'a', encoding=\"utf-8\")\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames, lineterminator='\\n')\n",
    "    print(gw)\n",
    "    if gw == 1:\n",
    "        writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "def collect_gw(gw, directory_name, output_dir):\n",
    "    rows = []\n",
    "    fieldnames = []\n",
    "    for root, dirs, files in os.walk(u\"./\" + directory_name):\n",
    "        for fname in files:\n",
    "            if fname == 'gw.csv':\n",
    "                fpath = os.path.join(root, fname)\n",
    "                fin = open(fpath, 'rU')\n",
    "                reader = csv.DictReader(fin)\n",
    "                fieldnames = reader.fieldnames\n",
    "                for row in reader:\n",
    "                    if int(row['round']) == gw:\n",
    "                        row['name'] = os.path.basename(root)\n",
    "                        rows += [row]\n",
    "\n",
    "    fieldnames = ['name'] + fieldnames\n",
    "    outf = open(os.path.join(output_dir, \"gw\" + str(gw) + \".csv\"), 'w', encoding=\"utf-8\")\n",
    "    writer = csv.DictWriter(outf, fieldnames=fieldnames, lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "def collect_all_gws(directory_name, output_dir):\n",
    "    for i in range(1,5):\n",
    "        collect_gw(i, directory_name, output_dir)\n",
    "\n",
    "def merge_all_gws(num_gws, gw_directory):\n",
    "    for i in range(1, num_gws):\n",
    "        merge_gw(i, gw_directory)\n",
    "\n",
    "# //def main():\n",
    "#     #collect_all_gws(sys.argv[1], sys.argv[2])\n",
    "#     merge_all_gws(int(sys.argv[1]), sys.argv[2])\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_data2(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Response was code \" + str(response.status_code))\n",
    "    html = response.text\n",
    "    parsed_html = BeautifulSoup(html, 'html.parser')\n",
    "    scripts = parsed_html.findAll('script')\n",
    "    filtered_scripts = []\n",
    "    for script in scripts:\n",
    "        if len(script.contents) > 0:\n",
    "            filtered_scripts += [script]\n",
    "    return scripts\n",
    "\n",
    "def get_epl_data():\n",
    "    scripts = get_data2(\"https://understat.com/league/EPL/2019\")\n",
    "    teamData = {}\n",
    "    playerData = {}\n",
    "    for script in scripts:\n",
    "        for c in script.contents:\n",
    "            split_data = c.split('=')\n",
    "            data = split_data[0].strip()\n",
    "            if data == 'var teamsData':\n",
    "                content = re.findall(r'JSON\\.parse\\(\\'(.*)\\'\\)',split_data[1])\n",
    "                decoded_content = codecs.escape_decode(content[0], \"hex\")[0].decode('utf-8')\n",
    "                teamData = json.loads(decoded_content)\n",
    "            elif data == 'var playersData':\n",
    "                content = re.findall(r'JSON\\.parse\\(\\'(.*)\\'\\)',split_data[1])\n",
    "                decoded_content = codecs.escape_decode(content[0], \"hex\")[0].decode('utf-8')\n",
    "                playerData = json.loads(decoded_content)\n",
    "    return teamData, playerData\n",
    "\n",
    "def parse_epl_data(outfile_base):\n",
    "    teamData,playerData = get_epl_data()\n",
    "    new_team_data = []\n",
    "    for t,v in teamData.items():\n",
    "        new_team_data += [v]\n",
    "    for data in new_team_data:\n",
    "        team_frame = pd.DataFrame.from_records(data[\"history\"])\n",
    "        team = data[\"title\"].replace(' ', '_')\n",
    "        team_frame.to_csv(os.path.join(outfile_base, 'understat_' + team + '.csv'), index=False)\n",
    "    player_frame = pd.DataFrame.from_records(playerData)\n",
    "    player_frame.to_csv(os.path.join(outfile_base, 'understat_player.csv'), index=False)\n",
    "\n",
    "# def main():\n",
    "#     parse_epl_data('data/2019-20/understat')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data\n",
      "Parsing summary data\n",
      "Cleaning summary data\n",
      "Getting fixtures data\n",
      "Getting teams data\n",
      "Extracting player ids\n",
      "Extracting player specific data\n",
      "Collecting gw scores\n",
      "Merging gw scores\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: DeprecationWarning: 'U' mode is deprecated\n",
      "/home/jordan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: 'U' mode is deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def parse_data():\n",
    "    \"\"\" Parse and store all the data\n",
    "    \"\"\"\n",
    "    print(\"Getting data\")\n",
    "    data = get_data()\n",
    "    season = '2020-21'\n",
    "    base_filename = 'data/' + season + '/'\n",
    "    print(\"Parsing summary data\")\n",
    "    parse_players(data[\"elements\"], base_filename)\n",
    "    events = data[\"events\"]\n",
    "    for event in events:\n",
    "        if event[\"is_current\"] == True:\n",
    "            gw_num = event[\"id\"]\n",
    "#     gw_num = 2\n",
    "    print(\"Cleaning summary data\")\n",
    "    clean_players(base_filename + 'players_raw.csv', base_filename)\n",
    "    print(\"Getting fixtures data\")\n",
    "    fixtures(base_filename)\n",
    "    print(\"Getting teams data\")\n",
    "    parse_team_data(data[\"teams\"], base_filename)\n",
    "    print(\"Extracting player ids\")\n",
    "    id_players(base_filename + 'players_raw.csv', base_filename)\n",
    "    player_ids = get_player_ids(base_filename)\n",
    "    num_players = len(data[\"elements\"])\n",
    "    player_base_filename = base_filename + 'players/'\n",
    "    gw_base_filename = base_filename + 'gws/'\n",
    "    print(\"Extracting player specific data\")\n",
    "    for i,name in player_ids.items():\n",
    "        player_data = get_individual_player_data(i)\n",
    "        parse_player_history(player_data[\"history_past\"], player_base_filename, name, i)\n",
    "        parse_player_gw_history(player_data[\"history\"], player_base_filename, name, i)\n",
    "    if gw_num > 0:\n",
    "        print(\"Collecting gw scores\")\n",
    "        collect_gw(gw_num, player_base_filename, gw_base_filename) \n",
    "        print(\"Merging gw scores\")\n",
    "        merge_gw(gw_num, gw_base_filename)\n",
    "#     understat_filename = base_filename + 'understat'\n",
    "#     parse_epl_data(understat_filename)\n",
    "\n",
    "def fixtures(base_filename):\n",
    "    data = get_fixtures_data()\n",
    "    parse_fixtures(data, base_filename)\n",
    "\n",
    "def main():\n",
    "    parse_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
